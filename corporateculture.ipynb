{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.models.word2vec as w2v\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import string\n",
    "import ace_tools as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to D:\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords', download_dir=r'D:\\nltk_data')\n",
    "nltk.data.path.append(r'D:\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Input working directory\n",
    "data_folder = r'D:/wrdsTables/ciqtranscriptcomponent_chunks_scored'\n",
    "\n",
    "# Set output directory for processed CSV files\n",
    "output_directory = r'D:/wrdsTables/corporate_culture_analysis'\n",
    "os.makedirs(output_directory, exist_ok=True) # create the folder if it does not exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]\n",
    "print(f\"Found {len(csv_files)} CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to load all CSV files into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files(folder, file_list):\n",
    "    dataframes = []\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(folder, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['source_file'] = file\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Load all files\n",
    "df = load_csv_files(data_folder, csv_files)\n",
    "\n",
    "print(f\"Total rows loaded: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache stopwords list as a set for faster lookup:\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define text pre processing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df['tokens'] = df['componenttext'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Word2Vec on All Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized text into a list of sentences\n",
    "sentences = df['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = w2v.Word2Vec(sentences, vector_size=300, window=5, min_count=5, workers = 2)\n",
    "# Vector_size = 300 dimensions to each vector\n",
    "# window = 5 needs clarification\n",
    "# min_count = 5 means words that apper fewer than 5 times cannot be included\n",
    "# workers = 2 is for processing, should be number of processors you have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save('w2v_corpculture.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate the Corporate Culture Dictionary\n",
    "###### Pulled the top 30 words associated with each value from Li et al Internet Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cultural seed words\n",
    "culture_values = {\n",
    "    \"innovation\" : [\"brand\", \"technology\", \"focus\", \"great\", \"platform\", \"ability\", \"best\", \"design\", \"create\", \"solution\", \"develop\", \"success\", \"content\", \"capability\", \"effort\", \"successful\", \"efficiency\", \"productivity\", \"learn\", \"unique\", \"tool\", \"innovation\", \"efficient\", \"terrific\", \"execution\", \"exciting\", \"enhance\", \"business_model\", \"enable\", \"discipline\"],\n",
    "    \"integrity\" : [\"control\", \"management\", \"careful\", \"honestly\", \"regulator\", \"honest\", \"safety\", \"assure\", \"compliance\", \"trust\", \"disciplined\", \"responsible\", \"proper\", \"responsibility\", \"thoughtful\", \"convince\", \"seriously\", \"transparent\", \"expert\", \"consistency\", \"candidly\", \"transparency\", \"authority\", \"responsive\", \"truth\", \"principle\", \"comply\", \"board_director\", \"thorough\", \"conflict\"],\n",
    "    \"quality\" : [\"customer\", \"product\", \"client\", \"service\", \"build\", \"deliver\", \"network\", \"support\", \"quality\", \"sales_force\", \"infrastructure\", \"supplier\", \"serve\", \"commit\", \"field\", \"commitment\", \"delivery\", \"vendor\", \"customer_base\", \"supply_chain\", \"critical\", \"requirement\", \"ensure\", \"speed\", \"desire\", \"productive\", \"guest\", \"service_provider\", \"capable\", \"functionality\"],\n",
    "    \"respect\" : [\"people\", \"team\", \"company\", \"hire\", \"folk\", \"organization\", \"resource\", \"employee\", \"management_team\", \"train\", \"training\", \"senior\", \"staff\", \"member\", \"leader\", \"person\", \"proud\", \"talent\", \"leadership\", \"manager\", \"ceo\", \"knowledge\", \"engineer\", \"recruit\", \"salespeople\", \"sales_team\", \"consultant\", \"culture\", \"sales_organization\", \"advisor\"],\n",
    "    \"teamwork\" : [\"partner\", \"relationship\", \"discussion\", \"together\", \"integrate\", \"involve\", \"conversation\", \"integration\", \"partnership\", \"engage\", \"align\", \"explore\", \"communication\", \"dialogue\", \"engagement\", \"contact\", \"conduct\", \"on_behalf_of\", \"joint\", \"collaboration\", \"sponsor\", \"conjunction\", \"supportive\", \"alliance\", \"merge\", \"interaction\", \"put_together\", \"organize\", \"embrace\", \"assist\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to get similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_culture_dictionary(model, culture_values, top_n=50):\n",
    "    culture_dictionary = {}\n",
    "    for value, seed_words in culture_values.items():\n",
    "        similar_words = []\n",
    "        for seed in seed_words:\n",
    "            if seed in model.wv:\n",
    "                similar_words += [word for word, _ in model.wv.most_similar(seed, topn=top_n)]\n",
    "        culture_dictionary[value] = list(set(similar_words))\n",
    "    return culture_dictionary\n",
    "\n",
    "# Generate dictionary\n",
    "culture_dictionary = generate_culture_dictionary(word2vec_model, culture_values)\n",
    "\n",
    "# Display first few words for each category\n",
    "for key, words in culture_dictionary.items():\n",
    "    print(f\"\\n{key.upper()}: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert tokenized text back into a corpus format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['tokens'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute TF-IDF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['processed_text'])\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, \n",
    "                                               columns=tfidf_feature_names, \n",
    "                                               index=df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute Culture Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_culture_scores(tfidf_df, culture_dictionary):\n",
    "    scores = {}\n",
    "    for value, words in culture_dictionary.items():\n",
    "        relevant_words = [word for word in words if word in tfidf_df.columns]\n",
    "        if relevant_words:\n",
    "            scores[value] = tfidf_df[relevant_words].sum(axis=1)\n",
    "        else:\n",
    "            scores[value] = np.zeros(len(tfidf_df))\n",
    "    return pd.DataFrame(scores, index=tfidf_df.index)\n",
    "\n",
    "# Generate culture scores\n",
    "culture_scores = compute_culture_scores(tfidf_df, culture_dictionary)\n",
    "\n",
    "# Merge with original dataset\n",
    "df = pd.concat([df, culture_scores], axis=1)\n",
    "\n",
    "# Save results\n",
    "df.to_csv(\"corporate_culture_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
